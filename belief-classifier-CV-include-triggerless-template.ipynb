{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated data directory\n",
    "annotated_data = \"/home/u32/cchyland/iBelieveFiles/only_uganda_with_triggers\"\n",
    "# original triggerless data directory\n",
    "triggerless_original_dir = \"/home/u32/cchyland/iBelieveFiles/triggerless_original/only_uganda\"\n",
    "# triggerless sample data (change directory for new samples); with header and labeled as \"n\" (non-belief)\n",
    "triggerless_sample = \"/home/u32/cchyland/iBelieveFiles/triggerless_sample/only_uganda\"\n",
    "# triggerless samples actually used\n",
    "triggerless_samples_used = \"/home/u32/cchyland/iBelieveFiles/triggerless_samples_used/only_uganda\"\n",
    "# save models here (change dir for newly-trained models):\n",
    "models_dir = \"/xdisk/msurdeanu/cchyland/models\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample of triggerless examples to use; if don't have too many, just use all of them (frac = 1.0)\n",
    "# percentage of triggerless sentences to sample if there are too many triggerless examples available:\n",
    "frac = 1.0\n",
    "random_seed = 22\n",
    "\n",
    "# this is what annotated column is called in the annotated data files---use the same names to assign annotations\n",
    "# to sampled negative examples:\n",
    "annotations_column = \"annotation: b (belief or attitude), n (not a belief and not an attitude)\"\n",
    "\n",
    "# TODO: to experiment with different sizes of triggerless examples, need to sample in some other way because it should\n",
    "# be based on the number of annotated examples, e.g., 4 times annotated examples;\n",
    "# could oversample and then sample from there?..\n",
    "for file in listdir(triggerless_original_dir):\n",
    "    f_path = os.path.join(triggerless_original_dir, file)\n",
    "    print(f_path)\n",
    "    temp_df = pd.read_csv(f_path, sep='\\t', header=None, on_bad_lines=\"skip\")\n",
    "#     print(temp_df.head())\n",
    "    print(len(temp_df))\n",
    "    # naming triggerless docs columns for easier use later\n",
    "    temp_df.columns = [\"file\", \"na\", \"sentence\", \"trigger\", \"na\", \"paragraph\",\"na\"]\n",
    "    temp_df[annotations_column] = [\"n\"] * len(temp_df)\n",
    "    temp_df.sample(frac=frac, random_state = random_seed).reset_index(drop=True).to_csv(os.path.join(triggerless_sample, file), sep=\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated_data = os.path.join(project_dir, \"annotated_as_of_dec13_both_uganda_and_rice\")\n",
    "# annotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(os.path.join(annotated_data, \"Subtask1-MainTask-double-annotation-prep-as-of-Nov2.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated data\n",
    "adf = pd.DataFrame()\n",
    "for file in listdir(annotated_data):\n",
    "    print(file)\n",
    "    if file.endswith(\"tsv\"):\n",
    "        f_path = os.path.join(annotated_data, file)\n",
    "        temp_df = pd.read_csv(f_path, sep='\\t', usecols = [\"paragraph\", \"mention text (just a few words around the trigger)\",\"trigger\",\"sentence\",\"annotation: b (belief or attitude), n (not a belief and not an attitude)\"]).dropna()\n",
    "        print(len(temp_df))\n",
    "        adf = pd.concat([adf, temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf[\"sentence\"] = [s.strip() for s in adf[\"sentence\"]]\n",
    "adf = adf.drop_duplicates(subset = [\"sentence\", \"mention text (just a few words around the trigger)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = adf[annotations_column]\n",
    "b_count = list(anns).count(\"b\")\n",
    "\n",
    "# percentage of sentences annotated as beliefs (among all annotated)\n",
    "float(b_count)/len(adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load negative examples sampled\n",
    "ndf = pd.DataFrame() \n",
    "for file in listdir(triggerless_sample):\n",
    "    f_path = os.path.join(triggerless_sample, file)\n",
    "    temp_df = pd.read_csv(f_path, sep='\\t', usecols = [\"paragraph\",\"trigger\",\"sentence\",annotations_column])\n",
    "#     print(len(temp_df))\n",
    "    ndf = pd.concat([ndf, temp_df])\n",
    "    \n",
    "ndf[\"sentence\"] = [s.strip() for s in ndf[\"sentence\"]]\n",
    "ndf = ndf.drop_duplicates(subset = [\"sentence\"])\n",
    "len(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times more triggerless data to use compared to trigger-ed examples\n",
    "# pick the number that is either the amount we want based on the multiplier or if that number is higher than the number\n",
    "# of available examples, just use all triggerless examples\n",
    "neg_example_multiplier = 2\n",
    "n_neg_examples_to_use = min(len(adf) * neg_example_multiplier, len(ndf))\n",
    "n_neg_examples_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the neg example sample, write it to a file for record keeping, and read it back in\n",
    "sample_file_name = os.path.join(triggerless_samples_used, f\"triggerless_sample_{neg_example_multiplier}_times_triggered_size\")\n",
    "rewrite_sample = True\n",
    "if path.exists(sample_file_name) and not rewrite_sample:\n",
    "    print(\"exists\")\n",
    "    ndf = pd.read_csv(sample_file_name, sep=\"\\t\")\n",
    "else:\n",
    "    print(\"new sample\")\n",
    "    ndf = ndf.sample(n=n_neg_examples_to_use, random_state = random_seed).reset_index(drop=True).to_csv(sample_file_name, index=False, sep=\"\\t\")\n",
    "    ndf = pd.read_csv(sample_file_name, sep=\"\\t\")\n",
    "    \n",
    "print(f\"N triggerless examples: {len(ndf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated + sampled triggerless\n",
    "df = pd.concat([adf, ndf])#.reset_index(drop=True)\n",
    "print(f\"Annotated + sampled = {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = [x for x in range(0,len(df.index))]\n",
    "\n",
    "# Adding markers to trigger\n",
    "for i in df.index: \n",
    "   if (not pd.isna(df.at[i,\"trigger\"])): \n",
    "       triggerText = df.at[i,\"trigger\"]\n",
    "       df.at[i,\"trigger\"] = df.at[i,\"trigger\"].replace(df.at[i,\"trigger\"], \"<t>\" + df.at[i,\"trigger\"] + \"</t>\")\n",
    "       df.at[i,\"sentence\"] = df.at[i,\"sentence\"].replace(triggerText, \"<t>\" + triggerText + \"</t>\")\n",
    "       df.at[i,\"paragraph\"] = df.at[i,\"paragraph\"].replace(triggerText, \"<t>\" + triggerText + \"</t>\")\n",
    "       df.at[i,\"mention text (just a few words around the trigger)\"] = df.at[i,\"mention text (just a few words around the trigger)\"].replace(triggerText, \"<t>\" + triggerText + \"</t>\")\n",
    "\n",
    "# assign numerical labels\n",
    "num_of_labels = len(list(set(df[annotations_column])))\n",
    "if num_of_labels == 2:\n",
    "    df['label'] = np.array([1 if x == \"b\" else 0 for x in df['annotation: b (belief or attitude), n (not a belief and not an attitude)']])\n",
    "else:\n",
    "    print(f\"Wrong number of labels: {number_of_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "# NOTE: for cross validation, the model should be initialized inside the cv loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_true = eval_pred.label_ids\n",
    "    y_pred = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    report = metrics.classification_report(y_true, y_pred)\n",
    "    print(\"report: \\n\", report)\n",
    "    \n",
    "    print(\"rep type: \", type(report))\n",
    "    \n",
    "\n",
    "    return {'f1':metrics.f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: not used right now, but can be\n",
    "# https://github.com/huggingface/transformers/blob/65659a29cf5a079842e61a63d57fa24474288998/src/transformers/models/bert/modeling_bert.py#L1486\n",
    "\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs,\n",
    "        )\n",
    "        cls_outputs = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_outputs = self.dropout(cls_outputs)\n",
    "        logits = self.classifier(cls_outputs)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for creating cross-validation folds\n",
    "def get_sample_based_on_idx(data, indeces):\n",
    "    return data.iloc[indeces, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 in set(df[\"label\"]) and 1 in set(df[\"label\"]) and len(list(set(df[\"label\"]))) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sentences as text; TODO: can add wrapping for trigger \n",
    "df[\"text\"] = df[\"sentence\"]\n",
    "# how much of the data to use (can limit number of debugging)\n",
    "df = df[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just checking the df looks right\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparams\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "weight_decay = 0.01\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_triggerless\", # is this location in the tmp dir? \n",
    "    log_level='error',\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    weight_decay=weight_decay,\n",
    "    load_best_model_at_end=True, # this is supposed to make sure the best model is loaded by the trainer at the end\n",
    "    metric_for_best_model=\"eval_f1\" \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"original.txt\", \"a\")\n",
    "\n",
    "fold = 0\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for train_df_idx, eval_df_idx in kfold.split(df):\n",
    "    \n",
    "    print(\"FOLD: \", fold)\n",
    "    output.write(f\"FOLD: {fold}\\n\")\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    train_df = get_sample_based_on_idx(df, train_df_idx)\n",
    "    print(\"LEN DF: \", len(train_df))\n",
    "    output.write(f\"LEN DF: {len(train_df)}\\n\")\n",
    "#     train_df['label'] = [int(item) for item in train_df[\"annotation: b (belief or attitude), n (not a belief and not an attitude)\"]]\n",
    "    print(\"done train df\")\n",
    "    output.write(\"done train df\\n\")\n",
    "    eval_df = get_sample_based_on_idx(df, eval_df_idx)\n",
    "#     eval_df[\"label\"] = [int(item) for item in eval_df['annotation: b (belief or attitude), n (not a belief and not an attitude)']]\n",
    "    print(\"done eval df\")\n",
    "    output.write(\"done eval df\\n\")\n",
    "    print(\"LEN EVAL: \", len(eval_df))\n",
    "    output.write(f\"LEN EVAL: {len(eval_df)}\\n\")\n",
    "#     print(eval_df.head())\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(train_df)\n",
    "    ds['validation'] = Dataset.from_pandas(eval_df)\n",
    "    train_ds = ds['train'].map(\n",
    "        tokenize, batched=True,\n",
    "        remove_columns=['index', 'sentence', 'trigger', 'annotation: b (belief or attitude), n (not a belief and not an attitude)', 'paragraph'],\n",
    "    )\n",
    "    eval_ds = ds['validation'].map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=['index', 'sentence', 'trigger', 'annotation: b (belief or attitude), n (not a belief and not an attitude)', 'paragraph'],\n",
    "    )\n",
    "\n",
    "#     config = AutoConfig.from_pretrained(\n",
    "#         transformer_name,\n",
    "#         num_labels=2,\n",
    "#     )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(transformer_name, num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "#     model = (\n",
    "#         BertForSequenceClassification\n",
    "#         .from_pretrained(transformer_name, config=config)\n",
    "#     )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "    # after training, predict (will use best model?)\n",
    "    preds = trainer.predict(eval_ds)\n",
    "#     print(\"HERE: \" , preds)\n",
    "    final_preds = [np.argmax(x) for x in preds.predictions]\n",
    "    real_f1 = metrics.f1_score(final_preds, eval_df[\"label\"])\n",
    "    print(\"F-1: \", real_f1)\n",
    "    output.write(f\"F-1: {real_f1}\\n\")\n",
    "    model_name = f\"{transformer_name}-best-of-fold-{fold}-f1-{real_f1}\"\n",
    "    model_dir = os.path.join(models_dir, model_name)\n",
    "\n",
    "    trainer.save_model(model_dir)\n",
    "    count_f_n = 0\n",
    "    count_f_p = 0\n",
    "    for i, item in enumerate(final_preds):\n",
    "        if not item == eval_ds[\"label\"][i]:\n",
    "            false_df = pd.DataFrame()\n",
    "            false_df[\"sentence\"] = [eval_df[\"sentence\"][i]]\n",
    "            false_df[\"real\"] = [eval_df[\"label\"][i]]\n",
    "            false_df[\"predicted\"] = [item]\n",
    "            new_df = pd.concat([new_df, false_df])\n",
    "#             print(\"NEW: \\n\", false_df.head())\n",
    "            if item == 0:\n",
    "                count_f_n += 1\n",
    "\n",
    "            else:\n",
    "                count_f_p += 1\n",
    "#                 print(eval_ds[\"sentence\"][i], \" \" , eval_ds[\"label\"][i], \" \", item, \"\\n\")\n",
    "\n",
    "    #     else:\n",
    "    #         print(\">>>\", list(X_test)[i], \" \" , y_test_enc[i], \" \", list(y_test)[i], \" \", item, \"\\n\")\n",
    "    print(f\"n of fasle pos: {count_f_n}\")\n",
    "    output.write(f\"n of fasle pos: {count_f_n}\\n\")\n",
    "    print(f\"n of false neg: {count_f_p}\")\n",
    "    output.write(f\"n of false neg: {count_f_p}\\n\")\n",
    "    \n",
    "    \n",
    "#     print(new_df.head())\n",
    "    new_df.to_csv(os.path.join(models_dir, \"false_annotations_\" + str(fold) + \".tsv\"), sep=\"\\t\")  \n",
    "    fold += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "output.write(\"{torch.cuda.memory_summary(device=None, abbreviated=False)}\")\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
